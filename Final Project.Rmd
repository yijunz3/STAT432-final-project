---
title: "STAT432 final project"
output: pdf_document
author: Yijun Zhao (yijunz3), Peiyi Chen (peiyic2), Rongxin Ni (rni4)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Used packages
library(dplyr)
library(tidyselect)
library(psych)
library(MASS)
library(glmnet)
library(ROCR)
library(e1071)
library(caret)
library(randomForest)
```


## Readin the Data

```{r}
  dataset = read.csv("brca_data_w_subtypes.csv")
  str(dataset)
  dim(dataset)
```

From the result, we know the data set contains 1941 variables and 705 observations.

```{r}
  # discard the vital.status variable
  data = subset(dataset, select = c(-vital.status))
  
  # separate outcomes from the data
  data_x = subset(data, select = c(-PR.Status, -ER.Status, 
                                               -HER2.Final.Status, 
                                               -histological.type))
  data_y = subset(data, select = c(PR.Status, ER.Status, 
                                               HER2.Final.Status, 
                                               histological.type))
  dim(data_x)
  dim(data_y)
```

## Summary Statistics and data processing

```{r}
  # Provide a summary of your data using univariate analysis
  summary = describe(data_x)
  summary
```


Based on the description of data, we realize that there are four different omics data types: cn-copy number variations (n=860), mu-mutations (n=249), rs-gene expression (n=604) and pp-protein levels (n=223). Variables in the types of rs and pp are continous, while the rest of variables are categorical.

According to the article "Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer", we know that mutations PTEN, TBX3, and FOXA1 are ILC enriched features, so these variables should definitely be included in our dataset. Moreover, there are Recurrently Mutated Genes in Breast Cancer given by this article, which should also be considered. We can remove other unrelated variables.

```{r}
data_continous = data_x %>%
  dplyr::select(matches("rs_"),matches("pp_"))

list = c('mu_PIK3CA','mu_RUNX1','mu_CDH1','mu_TP53','mu_TBX3','mu_PTEN','mu_FOXA1','mu_MAP3K1','mu_GATA3','mu_AKT1',"mu_NBL1",'mu_KMT2C','mu_DCTD','mu_RB1','mu_SF3B1','mu_CBFB','mu_ARHGAP35','mu_OR9A2','mu_NCOA3','mu_RBMX','mu_MAP2K4','mu_TROVE2','mu_NADK','mu_CASP8','mu_CTSS','mu_ACTL6B','mu_LGALS1','mu_KRAS','mu_KCNN3','mu_FBXW7','mu_LRIG2','mu_PIK3R1','mu_PARP4','mu_ZNF28','mu_HLA-DRB1','mu_ERBB2','mu_ZMYM3','mu_RAB42','mu_CTCF','mu_ATAD2','mu_CDKN1B','mu_GRIA2','mu_NCOR1','mu_HRNR','mu_GPRIN2','mu_PAX2','mu_ACTG1','mu_AQP12A','mu_PIK3C3','mu_MYB','mu_IRS4','mu_TBL1XR1','mu_RPGR','mu_CCNI','mu_ARID1A','mu_CD3EAP','mu_ADAMTS6','mu_OR2D2','mu_TMEM199','mu_MST1','mu_RHBG','mu_ZFP36L1','mu_TCP11','mu_CASZ1','mu_GAL3ST1','mu_FRMPD2','mu_GPS2','mu_ZNF362')

data_cat_1 = data_x %>%
  dplyr::select(matches(list))
data_cat_2 = data_x %>%
  dplyr::select(matches("cn_"))

data_cat = cbind(data_cat_1,data_cat_2)
dim(data_continous)
dim(data_cat)
```

If skewness is between -0.5 and 0.5, we consider the distribution to be approximately symmetric. Otherwise, if the absolute value of skewness is greater than 1, we consider the distribution to be highly skewed; if the absolute value is between 0.5 and 1, we consider the distribution to be moderately skewed. 

```{r}
#Check how many continuous variables are skewed distributed
summary_continous = describe(data_continous)
sum(abs(summary_continous$skew) >= 1)
```
For these highly skewed variables, we would perform cuberoot transformation, since the data contains both positive, zero and negative values.

```{r}
list_skew = which(abs(summary_continous$skew) >= 1)
data_continous[,list_skew] = data_continous[,list_skew]^(1/3)
#View(data_continous)
sum(abs(describe(data_continous)$skew) >= 1)
```
For the left 26 variables, we want to remove them.

```{r}
list_still_skew = which(abs(describe(data_continous)$skew) >= 1)
data_continous <- data_continous[,-c(list_still_skew)]
dim(data_continous)
```

```{r}

#check and remove the variables containing missing values
list_missing = c()
for (i in 1:dim(data_continous)[2]){
  if (sum(is.na(data_continous[,i])) != 0){
    list_missing <- append(list_missing,i)
  }
}
data_continous <- data_continous[,-c(list_missing)]
sum(is.na(data_continous))

#Check for outlier values as well
outliers = boxplot(data_continous)$out
outlier_num = length(boxplot(data_continous)$out)
outlier_num

dim(data_continous)

```

Notice that we have 5823 outliers. However, since those outliers only take 1% of all the values, and we think these outliers are more likely to come from variation instead of typo or incorrect measurement, we decide not to drop them.

We want to standardize our data to make variables comparable.
```{r}
data_continous <- scale(data_continous, center = TRUE, scale = TRUE)
```


Covariate imbalance may introduces bias into our model. We want to remove those extremely imbalanced classes (that is, the ratio between this class with others is over 10). From the previous result, we only want to keep classes 1,-1 and 0 for variables in the type "cn".

Let's look at the variables in the type "mu". Since they only have two classes 0 and 1, we will look for the ratio of these two classes in each variable, and then remove the ones that are very imbalanced.

```{r}
data_mu = data_cat %>%
  dplyr::select(matches("mu_"))

list_remove = c()
for (i in 1:dim(data_mu)[2]){
  if ((table(data_mu[,i])[1]/table(data_mu[,i])[2]) > 45){
      list_remove = append(list_remove,i)
  }
}

list_remove = colnames(data_mu)[list_remove]
```


```{r}
length(list_remove)
list_remove
```

```{r}

#data_cn = data_cat %>%
  #dplyr::select(matches("cn_"))

data_cat = data_cat[,-which(colnames(data_cat) %in% list_remove)]
dim(data_cat)
```


```{r}
#Generate the data of covariates
  data_x = cbind(data_continous,data_cat)
  # separate train & test data
  set.seed(12345)
  # 75% of the sample size
  smp_size <- floor(0.75 * nrow(data))
  train_idxes <- sample(seq_len(nrow(data)), size = smp_size, replace = FALSE, prob = NULL)
  train_data_x <- data_x[train_idxes,]
  test_data_x <- data_x[-train_idxes,]
  train_data_y <- data_y[train_idxes,]
  test_data_y <- data_y[-train_idxes,]
  
  dim(train_data_x)
  dim(train_data_y)
  
  dim(test_data_x)
  dim(test_data_y)
  
  # preprocess by outcome value
  preprocess <- function(data_x, data_y_type, 
                          class1="Positive", class2="Negative") {
    idxes = which(data_y_type %in% c(class1, class2))
    x = data_x[idxes, ]
    y = data_y_type[idxes]
    # class1 = 1, class2 = 0
    y = as.factor(y)
    y = as.numeric(y) - 1
    print(dim(x))
    print(length(y))
  
    return(list("x" = x,  "y" = y, "idxes"=idxes))
  }

```

## Build a classification model to predict PR.Status. 

Let's first take a look of the data.

```{r}
#data_pr = data %>%
  #select(-c('ER.Status'))

table(data$PR.Status)
```

There exists 122 missing values should be removed, classes "Indeterminate", "Not Performed" and "Performed but Not Available" contain few value, so we will only focus on classes "Positive" and "Negative". 

```{r}
  pr_data_train = preprocess(train_data_x, train_data_y$PR.Status)
  pr_data_test = preprocess(test_data_x, test_data_y$PR.Status)
  
```


### Approach 1 (lda)
```{r}

dig.lda = lda(pr_data_train$x,pr_data_train$y)

  # Use classification error as the evaluation criterion.
Ytest.pred = predict(dig.lda, pr_data_test$x)
mean(pr_data_test$y != Ytest.pred$class)  

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
table(pr_data_test$y, Ytest.pred$class)

```

### Approach 2 (Kmeans)
```{r}
set.seed(12345)
pr_mat_train = cbind(pr_data_train$x,pr_data_train$y)
kmeanfit <- kmeans(pr_mat_train[,-dim(pr_mat_train)[2]],2,25)


  # Use classification error as the evaluation criterion.
mean((pr_mat_train$`pr_data_train$y` + 1) != kmeanfit$cluster)

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
table(pr_mat_train$`pr_data_train$y`, kmeanfit$cluster)

```


## Build a classification model to predict histological.type 

```{r}
  hist_data_train = preprocess(train_data_x, 
                          train_data_y$histological.type, 
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
  hist_data_test = preprocess(test_data_x, 
                          test_data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
  
```
### Approach 1 (Logistic Regression)

```{r}
  logistic.fit <- glmnet(hist_data_train$x, hist_data_train$y, alpha = 0, family = "binomial")
  pred = predict(logistic.fit, data.matrix(hist_data_test$x), type = "response", s=min(logistic.fit$lambda))
  
  # Use AUC as the evaluation criterion.

  roc <- prediction(pred, hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]
  
  # calculates the ROC curve
  perf <- performance(roc,"tpr","fpr")
  plot(perf,colorize=TRUE)
  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred > 0.5, hist_data_test$y)
```

### Approach 2 (SVM)

```{r}

  svm.fit <- svm(as.factor(hist_data_train$y) ~ ., 
                  data = data.frame(hist_data_train$x), 
                  type='C-classification',
                  probability = TRUE,
                  kernel='linear', scale=FALSE, cost = 1)
  
  pred = predict(svm.fit, hist_data_test$x, probability=TRUE)
  pred_prob = attr(pred, "probabilities")
  
  # Use AUC as the evaluation criterion.
  roc <- prediction(pred_prob[,2], hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]
  
  # calculates the ROC curve
  perf <- performance(roc,"tpr","fpr")
  plot(perf,colorize=TRUE)
  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred_prob[,2] > 0.5, hist_data_test$y)
```

## Variable selection for all outcomes

```{r}
  # preprocess by outcome value
  pr_data = preprocess(data_x, data_y$PR.Status)

  er_data = preprocess(data_x, data_y$ER.Status)

  her_data = preprocess(data_x, data_y$HER2.Final.Status)
  
  hist_data = preprocess(data_x, data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
```

```{r}
  # tune rf parameters

  tunegrid <- expand.grid(mtry = c(50, 100, 500, 700),min.node.size = c(1,5,10,20),
                          splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(hist_data$y), 
                    x = hist_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, hist_data$x)
  
  roc <- prediction(as.numeric(pred), as.numeric(hist_data$y))
  performance(roc, measure = "auc")@y.values[[1]]
  
  table(pred, hist_data$y)
```

```{r}
  # select a total of 50 variables

  rf.fit_pr = randomForest(pr_data$x, as.factor(pr_data$y), ntree = 400, 
                        mtry = 500, nodesize = 1)
  rf.fit_er = randomForest(er_data$x, as.factor(er_data$y), ntree = 400, 
                        mtry = 500, nodesize = 1)
  rf.fit_her = randomForest(her_data$x, as.factor(her_data$y), ntree = 400, 
                        mtry = 500, nodesize = 1)
  rf.fit_hist = randomForest(hist_data$x, as.factor(hist_data$y), ntree = 400, 
                        mtry = 500, nodesize = 1)
  total_importance = rf.fit_pr$importance + rf.fit_er$importance + rf.fit_her$importance + rf.fit_hist$importance
  idxes = sort(total_importance, decreasing = TRUE, index.return=TRUE)$ix[1:50]
  pr_data$x = pr_data$x[idxes]
  er_data$x = er_data$x[idxes]
  her_data$x = her_data$x[idxes]
  hist_data$x = hist_data$x[idxes]
  dim(pr_data$x)
  dim(er_data$x)
  dim(her_data$x)
  dim(hist_data$x)
```

```{r}
  # evaluation criteria is based on a three-fold cross-validation with AUC for each outcome, and then average the cross-validated AUC of all four outcomes
  set.seed(1)
  all_fold_ids = sample(1:3, 705, replace = TRUE)
  
  eval <- function(data_x, data_y, fold_ids) {
    auc_list = c()
    for (i in 1:3) {
      test_idxes <- which(fold_ids==i)
      test_x <- data_x[test_idxes,]
      test_y <- data_y[test_idxes]
      train_x <- data_x[-test_idxes,]
      train_y <- data_y[-test_idxes]

      fit <- glmnet(train_x, as.factor(train_y), alpha = 0, family = "binomial")
      pred = predict(fit, data.matrix(test_x), type = "response", s=min(fit$lambda))
      roc <- prediction(pred, test_y)
      
      auc <- performance(roc, measure = "auc")@y.values[[1]]
      auc_list = c(auc_list, auc)
    }
    return(mean(auc_list))
  }

```


```{r}
  # must generate the fold ID (for all 705 observations) using the following code
  set.seed(1)
  fold_id_list = sample(1:3, 705, replace = TRUE)
  
  pr_auc = eval(pr_data$x, pr_data$y, all_fold_ids[pr_data$idxes])
  er_auc = eval(er_data$x, er_data$y, all_fold_ids[er_data$idxes])
  her_auc = eval(her_data$x, her_data$y, all_fold_ids[her_data$idxes])
  hist_auc = eval(hist_data$x, hist_data$y, all_fold_ids[hist_data$idxes])
  
  res = mean(c(pr_auc, hist_auc, er_auc, her_auc))
  
  pr_auc
  er_auc
  her_auc
  hist_auc
  res
```

```{r}
  colnames(pr_data$x)  
```
