---
title: "STAT432 final project"
output: pdf_document
author: Yijun Zhao (yijunz3), Peiyi Chen (peiyic2), Rongxin Ni (rni4)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(tidyselect)
```


## Readin the Data

```{r}
  dataset = read.csv("brca_data_w_subtypes.csv")
  dataset
```

```{r}
  dim(dataset)
  # discard the vital.status variable
  data = subset(dataset, select = c(-vital.status))
  
  # separate outcomes from the training variables
  data_x = subset(data, select = c(-PR.Status, -ER.Status, 
                                               -HER2.Final.Status, 
                                               -histological.type))
  data_y = subset(data, select = c(PR.Status, ER.Status, 
                                               HER2.Final.Status, 
                                               histological.type))
  dim(data_x)
  dim(data_y)
  data_y
```

## Summary Statistics and data processing

```{r}
  # Provide a summary of your data using univariate analysis
  library(psych)
  summary = describe(data_x)
  summary
```

Based on the description of data, we realize that there are four different omics data types: cn-copy number variations (n=860), mu-mutations (n=249), rs-gene expression (n=604) and pp-protein levels (n=223). Variables in the types of rs and pp are continous, while the rest of variables are categorical.

According to the article "Comprehensive Molecular Portraits of Invasive Lobular Breast Cancer", we know that mutations PTEN, TBX3, and FOXA1 are ILC enriched features, so these variables should definitely be included in our dataset. Moreover, there are Recurrently Mutated Genes in Breast Cancer given by this article, which should also be considered. We can remove other unrelated variables.

```{r}
data_continous = data_x %>%
  select(matches("rs_"),matches("pp_"))

list = c('mu_PIK3CA','mu_RUNX1','mu_CDH1','mu_TP53','mu_TBX3','mu_PTEN','mu_FOXA1','mu_MAP3K1','mu_GATA3','mu_AKT1',"mu_NBL1",'mu_KMT2C','mu_DCTD','mu_RB1','mu_SF3B1','mu_CBFB','mu_ARHGAP35','mu_OR9A2','mu_NCOA3','mu_RBMX','mu_MAP2K4','mu_TROVE2','mu_NADK','mu_CASP8','mu_CTSS','mu_ACTL6B','mu_LGALS1','mu_KRAS','mu_KCNN3','mu_FBXW7','mu_LRIG2','mu_PIK3R1','mu_PARP4',')

data_cat_1 = data_x %>%
  dplyr::select(list)
data_cat_1 = 
data_cat_2 = data_x %>%
  select(matches("cn_"))

data_cat = cbind(data_cat_1,data_cat_2)
dim(data_continous)
dim(data_cat)
```



```{r}
# For continuous predictors, is there any outlier/missing value? Do you need to do any transformations?

#Check for missing values, 需要注意的是，不确定0是不是missing value
sum(is.na(data_continous))

#Check for outlier values
length(boxplot(data_continous)$out)



```

```{r}
#Check the histogram of first and last 5 continuous variables
for (i in c(1:5,822:827)) {
  hist(data_continous[,i], xlab = colnames(data_continous)[i])
}

```

Although some variables are skewed, we can observe that the majority of variables are normally distributed. Therefore, transformation is unnecessary.

```{r}
  # For continuous predictors, is there any outlier/missing value? Do you need to do any transformations?
  # which(colnames(data_x) == "cn_ISG15")
  ### 会hurt performance
  continuous_data = data_x[1:604]
  summary = describe(continuous_data)
  skew_idx = which(abs(summary$skew) > 1)
  length(skew_idx)
  # number of missing values
  nrow(continuous_data) - nrow(na.omit(continuous_data)) 
  # number of outlier values
  length(boxplot(continuous_data)$out)
  
  for (i in skew_idx) {
    shift = 0
    if (summary$min[i] <= 0) {
      shift = 1
    }
    #hist(continuous_data[,i], breaks = 10, main = colnames(continuous_data)[i])
    continuous_data[,i] <- log(shift + continuous_data[,i])
  }
  data_x = continuous_data
  
  
```

We have no missing values, but have many outliers. We would consider remove variables with too many outliers. 

There also exists many unbalanced categorical data, see the following examples.

```{r}
  # For categorical predictors, do you need to deal with variables that are extremely unbalanced?
  #categorical_data = data_x[,605:1936]
  for (i in 1:10){
    print(table(data_cat[,i]))
    
  }

#We can try to deal with the first unbalanced variable


```



```{r}
  # Any variable/observation you decided to remove from the analysis? And for what reason?



  # You need to provide tables and/or figures to properly display the information to support your decision and clearly document your processing steps.

```

```{r}
  # separate train & test data
  set.seed(12345)
  # 75% of the sample size
  smp_size <- floor(0.75 * nrow(data))
  #test_idxes <- sample(nrow(realestate), 100, replace = FALSE, prob = NULL) 
  train_idxes <- sample(seq_len(nrow(data)), size = smp_size, replace = FALSE, prob = NULL)
  train_data_x <- data_x[train_idxes,]
  test_data_x <- data_x[-train_idxes,]
  train_data_y <- data_y[train_idxes,]
  test_data_y <- data_y[-train_idxes,]
  
  dim(train_data_x)
  dim(train_data_y)
  
  dim(test_data_x)
  dim(test_data_y)
  
  # preprocess by outcome value
  preprocess <- function(data_x, data_y_type, 
                          class1="Positive", class2="Negative") {
    idxes = which(data_y_type %in% c(class1, class2))
    x = data_x[idxes, ]
    y = data_y_type[idxes]
    # class1 = 1, class2 = 0
    y = as.factor(y)
    y = as.numeric(y) - 1
    print(dim(x))
    print(length(y))
  
    return(list("x" = x,  "y" = y))
  }

```

## Build a classification model to predict PR.Status. 

Let's first take a look of the data.

```{r}
#data_pr = data %>%
  #select(-c('ER.Status'))

table(data$PR.Status)
```

There exists 122 missing values should be removed, classes "Indeterminate", "Not Performed" and "Performed but Not Available" contain few value, so we will only focus on classes "Positive" and "Negative". 

```{r}
  pr_data_train = preprocess(train_data_x, train_data_y$PR.Status)
  pr_data_test = preprocess(test_data_x, test_data_y$PR.Status)
  
```


### Approach 1 (lda)
```{r}

library(MASS)
dig.lda = lda(pr_data_train$x,pr_data_train$y)
  # Use classification error as the evaluation criterion.
Ytest.pred = predict(dig.lda, pr_data_test$x)
mean(pr_data_test$y != Ytest.pred$class)  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
table(pr_data_test$y, Ytest.pred$class)

```

### Approach 2 (Kmeans)
```{r}
set.seed(12345)
pr_mat_train = cbind(pr_data_train$x,pr_data_train$y)
kmeanfit <- kmeans(pr_mat_train[,-1], 2)


  # Use classification error as the evaluation criterion.
mean((pr_mat_train$`pr_data_train$y` + 1) != kmeanfit$cluster)

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
table(pr_mat_train$`pr_data_train$y`, kmeanfit$cluster)

```

## Build a classification model to predict histological.type 

```{r}
  hist_data_train = preprocess(train_data_x, 
                          train_data_y$histological.type, 
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
  hist_data_test = preprocess(test_data_x, 
                          test_data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
```
### Approach 1 (Logistic Regression)
```{r}
  ## Approach 1 (should be different from the PR.Status models)
  logistic.fit <- glm(hist_data_train$y~., 
                      data = data.frame(hist_data_train$x), family = binomial)
  pred = predict(logistic.fit, newdata = hist_data_test$x, type = "response") 
  # Use AUC as the evaluation criterion.
  library(ROCR)
  roc <- prediction(pred, hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred > 0.5, hist_data_test$y)
```

### Approach 2 (SVM)

```{r}
  library(e1071)
  svm.fit <- svm(hist_data$train_y ~ ., 
                  data = data.frame(hist_data_train$x), 
                  type='C-classification',
                  probability = TRUE,
                  kernel='linear', scale=FALSE, cost = 1)
  
  pred = predict(svm.fit, hist_data_test$x, probability=TRUE) # confusion table
  pred_prob = attr(pred, "probabilities")
  
  # Use AUC as the evaluation criterion.
  roc <- prediction(pred_prob[,2], hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]
  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred_prob[,2] > 0.5, hist_data_test$y)
```

## Variable selection for all outcomes

```{r}
  # preprocess by outcome value
  pr_data = preprocess(data_x, data_y$PR.Status)

  er_data = preprocess(data_x, data_y$ER.Status)

  her_data = preprocess(data_x, data_y$HER2.Final.Status)
  
  hist_data = preprocess(data_x, data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
```

```{r}
  # select a total of 50 variables (You can consider reading relevant papers for this task and help guide your variable selection procedure. This means that your final model does not need to be completely data driven. It can be partially knowledge driven. If you do so, please clearly document your procedure, and you should also mention them in the literature review.)
  library(randomForest)
  rf.fit_pr = randomForest(pr_data$x, pr_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_er = randomForest(er_data$x, er_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_her = randomForest(her_data$x, her_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_hist = randomForest(hist_data$x, hist_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  total_importance = rf.fit_pr$importance + rf.fit_er$importance 
                      + rf.fit_her$importance + rf.fit_hist$importance
  idxes = sort(total_importance, decreasing = TRUE, index.return=TRUE)$ix[1:50]
  pr_data$x = pr_data$x[idxes]
  er_data$x = er_data$x[idxes]
  her_data$x = her_data$x[idxes]
  hist_data$x = hist_data$x[idxes]
  dim(pr_data$x)
  dim(er_data$x)
  dim(her_data$x)
  dim(hist_data$x)
```

```{r}
  # build models using only these variables to predict all four outcomes
  
  ## PR.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(pr_data$y), 
                    x = pr_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, pr_data$x)
  table(pred, pr_data$y)
```

```{r}
  ## ER.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50), min.node.size = c(1,5,10), splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(er_data$y), 
                    x = er_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, er_data$x)
  table(pred, er_data$y)
```

```{r}
  ## HER2.Final.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(her_data$y), 
                    x = her_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, her_data$x)
  table(pred, her_data$y)
```

```{r}
  ## histological.type
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(hist_data$y), 
                    x = hist_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, hist_data$x)
  table(pred, hist_data$y)
```


```{r}

  # evaluation criteria is based on a three-fold cross-validation with AUC for each outcome, and then average the cross-validated AUC of all four outcomes
  
  # must generate the fold ID (for all 705 observations) using the following code
  # set.seed(1); sample(1:3, 705, replace = TRUE)

```