---
title: "STAT432 final project"
output: pdf_document
author: Yijun Zhao (yijunz3), Peiyi Chen (peiyic2), Rongxin Ni (rni4)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Readin the Data

```{r}
  dataset = read.csv("brca_data_w_subtypes.csv")
  dataset
```

```{r}
  dim(dataset)
  # discard the vital.status variable
  data = subset(dataset, select = c(-vital.status))
  
  # separate outcomes from the training variables
  data_x = subset(data, select = c(-PR.Status, -ER.Status, 
                                               -HER2.Final.Status, 
                                               -histological.type))
  data_y = subset(data, select = c(PR.Status, ER.Status, 
                                               HER2.Final.Status, 
                                               histological.type))
  dim(data_x)
  dim(data_y)
  data_y
```

## Summary Statistics and data processing

```{r}
  # Provide a summary of your data using univariate analysis
  library(psych)
  summary = describe(data_x)
  summary
```

```{r}
  # For continuous predictors, is there any outlier/missing value? Do you need to do any transformations?
  # which(colnames(data_x) == "cn_ISG15")
  ### ä¼šhurt performance
  continuous_data = data_x[1:604]
  summary = describe(continuous_data)
  skew_idx = which(abs(summary$skew) > 1)
  length(skew_idx)
  # number of missing values
  nrow(continuous_data) - nrow(na.omit(continuous_data)) 
  # number of outlier values
  length(boxplot(continuous_data)$out)
  
  for (i in skew_idx) {
    shift = 0
    if (summary$min[i] <= 0) {
      shift = 1
    }
    #hist(continuous_data[,i], breaks = 10, main = colnames(continuous_data)[i])
    continuous_data[,i] <- log(shift + continuous_data[,i])
  }
  data_x = continuous_data
```
We have no missing values, but have many outliers

```{r}
  # For categorical predictors, do you need to deal with variables that are extremely unbalanced?
  categorical_data = data_x[,605:1936]
```

```{r}
  # Any variable/observation you decided to remove from the analysis? And for what reason?

  # You need to provide tables and/or figures to properly display the information to support your decision and clearly document your processing steps.

```

```{r}
  # separate train & test data
  set.seed(12345)
  # 75% of the sample size
  smp_size <- floor(0.75 * nrow(data))
  #test_idxes <- sample(nrow(realestate), 100, replace = FALSE, prob = NULL) 
  train_idxes <- sample(seq_len(nrow(data)), size = smp_size, replace = FALSE, prob = NULL)
  train_data_x <- data_x[train_idxes,]
  test_data_x <- data_x[-train_idxes,]
  train_data_y <- data_y[train_idxes,]
  test_data_y <- data_y[-train_idxes,]
  
  dim(train_data_x)
  dim(train_data_y)
  
  dim(test_data_x)
  dim(test_data_y)
  
  # preprocess by outcome value
  preprocess <- function(data_x, data_y_type, 
                          class1="Positive", class2="Negative") {
    idxes = which(data_y_type %in% c(class1, class2))
    x = data_x[idxes, ]
    y = data_y_type[idxes]
    # class1 = 1, class2 = 0
    y = as.factor(y)
    y = as.numeric(y) - 1
    print(dim(x))
    print(length(y))
  
    return(list("x" = x,  "y" = y))
  }

```

## Build a classification model to predict PR.Status. 

```{r}
  pr_data_train = preprocess(train_data_x, train_data_y$PR.Status)
  pr_data_test = preprocess(test_data_x, test_data_y$PR.Status)
```

```{r}
  ## Approach 1

  # Use classification error as the evaluation criterion.
  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results

```

```{r}
  ## Approach 2

  # Use classification error as the evaluation criterion.

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results

```

## Build a classification model to predict histological.type 

```{r}
  hist_data_train = preprocess(train_data_x, 
                          train_data_y$histological.type, 
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
  hist_data_test = preprocess(test_data_x, 
                          test_data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
```
### Approach 1 (Logistic Regression)
```{r}
  ## Approach 1 (should be different from the PR.Status models)
  logistic.fit <- glm(hist_data_train$y~., 
                      data = data.frame(hist_data_train$x), family = binomial)
  pred = predict(logistic.fit, newdata = hist_data_test$x, type = "response") 
  # Use AUC as the evaluation criterion.
  library(ROCR)
  roc <- prediction(pred, hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]

  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred > 0.5, hist_data_test$y)
```

### Approach 2 (SVM)

```{r}
  library(e1071)
  svm.fit <- svm(hist_data$train_y ~ ., 
                  data = data.frame(hist_data_train$x), 
                  type='C-classification',
                  probability = TRUE,
                  kernel='linear', scale=FALSE, cost = 1)
  
  pred = predict(svm.fit, hist_data_test$x, probability=TRUE) # confusion table
  pred_prob = attr(pred, "probabilities")
  
  # Use AUC as the evaluation criterion.
  roc <- prediction(pred_prob[,2], hist_data_test$y)
  performance(roc, measure = "auc")@y.values[[1]]
  
  # You need to provide sufficient information (table, figure and descriptions) to demonstrate the model fitting results
  table(pred_prob[,2] > 0.5, hist_data_test$y)
```

## Variable selection for all outcomes

```{r}
  # preprocess by outcome value
  pr_data = preprocess(data_x, data_y$PR.Status)

  er_data = preprocess(data_x, data_y$ER.Status)

  her_data = preprocess(data_x, data_y$HER2.Final.Status)
  
  hist_data = preprocess(data_x, data_y$histological.type,
                          class1 = "infiltrating lobular carcinoma", 
                          class2 = "infiltrating ductal carcinoma")
```

```{r}
  # select a total of 50 variables (You can consider reading relevant papers for this task and help guide your variable selection procedure. This means that your final model does not need to be completely data driven. It can be partially knowledge driven. If you do so, please clearly document your procedure, and you should also mention them in the literature review.)
  library(randomForest)
  rf.fit_pr = randomForest(pr_data$x, pr_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_er = randomForest(er_data$x, er_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_her = randomForest(her_data$x, her_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  rf.fit_hist = randomForest(hist_data$x, hist_data$y, ntree = 1000, 
                        mtry = 7, nodesize = 100, sampsize = 300)
  total_importance = rf.fit_pr$importance + rf.fit_er$importance 
                      + rf.fit_her$importance + rf.fit_hist$importance
  idxes = sort(total_importance, decreasing = TRUE, index.return=TRUE)$ix[1:50]
  pr_data$x = pr_data$x[idxes]
  er_data$x = er_data$x[idxes]
  her_data$x = her_data$x[idxes]
  hist_data$x = hist_data$x[idxes]
  dim(pr_data$x)
  dim(er_data$x)
  dim(her_data$x)
  dim(hist_data$x)
```

```{r}
  # build models using only these variables to predict all four outcomes
  
  ## PR.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(pr_data$y), 
                    x = pr_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, pr_data$x)
  table(pred, pr_data$y)
```

```{r}
  ## ER.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50), min.node.size = c(1,5,10), splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(er_data$y), 
                    x = er_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, er_data$x)
  table(pred, er_data$y)
```

```{r}
  ## HER2.Final.Status
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(her_data$y), 
                    x = her_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, her_data$x)
  table(pred, her_data$y)
```

```{r}
  ## histological.type
  library(caret)
  tunegrid <- expand.grid(mtry = c(5,10,50),min.node.size = c(1,5,10),splitrule = "gini")
  ctrl <- trainControl(method = "cv", number = 3)
  rf.fit = train(y=as.factor(hist_data$y), 
                    x = hist_data$x, method = 'ranger', 
                    trControl = ctrl, num.trees = 400, tuneGrid = tunegrid,
                    respect.unordered.factors = "partition")
  rf.fit
  pred = predict(rf.fit, hist_data$x)
  table(pred, hist_data$y)
```


```{r}

  # evaluation criteria is based on a three-fold cross-validation with AUC for each outcome, and then average the cross-validated AUC of all four outcomes
  
  # must generate the fold ID (for all 705 observations) using the following code
  # set.seed(1); sample(1:3, 705, replace = TRUE)

```